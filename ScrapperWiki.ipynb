{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets write a Simple script\n",
    "# to get the 20 top words and their frequency percentage\n",
    "# with highest frequency in an English Wikipedia article.\n",
    "\n",
    "#Beautiful Soup is a Python library\n",
    "#for pulling data out of HTML and XML files.\n",
    "from bs4 import BeautifulSoup\n",
    "#Requests is one of the most downloaded\n",
    "#Python packages of all time,\n",
    "#pulling in over 7,000,000 downloads every month.\n",
    "#HTTP library for pulling pushing and authenticating\n",
    "import requests\n",
    "#lets you do Regular expression operations\n",
    "#special text string for describing a search pattern.\n",
    "#find and replace\n",
    "import re\n",
    "#The operator module exports a\n",
    "#set of efficient functions\n",
    "#corresponding to the intrinsic operators of Python.\n",
    "#comparison, addition, greater than less then\n",
    "import operator\n",
    "#parses json, formats it\n",
    "import json\n",
    "#The module provides just one function,\n",
    "#tabulate, which takes a list of lists or another\n",
    "#tabular data type as the first argument,\n",
    "#and outputs a nicely formatted plain-text table:\n",
    "from tabulate import tabulate\n",
    "#system calls, dealw with user arguments\n",
    "import sys\n",
    "#list of common stop words various languages like the\n",
    "from stop_words import get_stop_words\n",
    "#random\n",
    "import random\n",
    "#science tools\n",
    "import numpy as np\n",
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the words\n",
    "def getWordList(url):\n",
    "    word_list = []\n",
    "    #raw data\n",
    "    source_code = requests.get(url)\n",
    "    #convert to text\n",
    "    plain_text = source_code.text\n",
    "    #lxml format\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "\n",
    "    #find the words in paragraph tag\n",
    "    for text in soup.findAll('p'):\n",
    "        if text.text is None:\n",
    "            continue\n",
    "        #content\n",
    "        content = text.text\n",
    "        #lowercase and split into an array\n",
    "        words = content.lower().split()\n",
    "\n",
    "        #for each word\n",
    "        for word in words:\n",
    "            #remove non-chars\n",
    "            cleaned_word = clean_word(word)\n",
    "            #if there is still something there\n",
    "            if len(cleaned_word) > 0:\n",
    "                #add it to our word list\n",
    "                word_list.append(cleaned_word)\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "#clean word with regex\n",
    "def clean_word(word):\n",
    "    cleaned_word = re.sub('[^A-Za-z]+', '', word)\n",
    "    return cleaned_word\n",
    "\n",
    "\n",
    "def createFrquencyTable(word_list):\n",
    "    #word count\n",
    "    never_count = 0\n",
    "    word_count = {}\n",
    "    for word in word_list:\n",
    "        #index is the word\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "\n",
    "    return word_count\n",
    "\n",
    "\n",
    "#remove stop words\n",
    "def remove_stop_words(frequency_list):\n",
    "    stop_words = get_stop_words('en')\n",
    "\n",
    "    temp_list = []\n",
    "    for key, value in frequency_list:\n",
    "        if key not in stop_words:\n",
    "            temp_list.append([key, value])\n",
    "\n",
    "    return temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word='dog'\n",
    "search_modeFlag='Yes' \n",
    "target_word='cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapWiki(search_word, search_modeFlag, target_word, printing):\n",
    "    #access wiki API. json format. query it for data. search tyep. shows list of possibilities\n",
    "    wikipedia_api_link = \"https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch=\"\n",
    "    wikipedia_link = \"https://en.wikipedia.org/wiki/\"\n",
    "    percentage_value_targeted = 0;\n",
    "    nextWord = search_word\n",
    "    \n",
    "    #if the search word is too small, throw error\n",
    "    if (len(search_word) < 2):\n",
    "        print(\"Enter valid string\")\n",
    "        return None, search_word\n",
    "\n",
    "    #get the search word\n",
    "    string_query = search_word\n",
    "\n",
    "    #to remove stop words or not\n",
    "    if (len(search_modeFlag) > 2):\n",
    "        search_mode = True\n",
    "    else:\n",
    "        search_mode = False\n",
    "\n",
    "    #create our URL\n",
    "    url = wikipedia_api_link + string_query\n",
    "\n",
    "    #try-except block. simple way to deal with exceptions\n",
    "    #great for HTTP requests\n",
    "    try:\n",
    "        #use requests to retrieve raw data from wiki API URL we\n",
    "        #just constructed\n",
    "        response = requests.get(url)\n",
    "\n",
    "        #format that data as a JSON dictionary\n",
    "        data = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "        #page title, first option\n",
    "        #show this in web browser\n",
    "        if data['query']['search'][1]['title'] == string_query:\n",
    "            wikipedia_page_tag = data['query']['search'][1]['title']\n",
    "        else:\n",
    "            wikipedia_page_tag = data['query']['search'][0]['title']\n",
    "\n",
    "        #get actual wiki page based on retrieved title\n",
    "        url = wikipedia_link + wikipedia_page_tag\n",
    "        print(\"The URL is \" + url)\n",
    "        \n",
    "        #get list of words from that page\n",
    "        page_word_list = getWordList(url)\n",
    "            \n",
    "        #create table of word counts, dictionary\n",
    "        page_word_count = createFrquencyTable(page_word_list)\n",
    "        #sort the table by the frequency count\n",
    "        sorted_word_frequency_list = sorted(\n",
    "            page_word_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #remove stop words if the user specified\n",
    "        if (search_mode):\n",
    "            sorted_word_frequency_list = remove_stop_words(\n",
    "                sorted_word_frequency_list)\n",
    "\n",
    "        #sum the total words to calculate frequencies\n",
    "        total_words_sum = 0\n",
    "        \n",
    "        for key, value in sorted_word_frequency_list:\n",
    "            total_words_sum = total_words_sum + value\n",
    " \n",
    "        #create our final list which contains words, frequency (word count), percentage\n",
    "        final_list = []\n",
    "        for key, value in sorted_word_frequency_list:\n",
    "            percentage_value = float(value * 100) / total_words_sum\n",
    "            final_list.append([key, value, round(percentage_value, 4)])\n",
    "            if key == target_word:\n",
    "                percentage_value_targeted = percentage_value\n",
    "                \n",
    "        nextWord = key\n",
    "                       \n",
    "        print('Freq of targeted word: ' + str(percentage_value_targeted))\n",
    "\n",
    "        #just get the top 20 words\n",
    "        if len(sorted_word_frequency_list) > 20:\n",
    "            final_list = final_list[:20]\n",
    "        \n",
    "        if (printing):                \n",
    "            #headers before the table\n",
    "            print_headers = ['Word', 'Frequency', 'Frequency Percentage']\n",
    "            #print the table with tabulate\n",
    "            print(tabulate(final_list, headers=print_headers, tablefmt='orgtbl'))\n",
    "        \n",
    "    #throw an exception in case it breaks\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The server didn't respond. This word has no page.\")\n",
    "        return None, search_word\n",
    "        \n",
    "    return percentage_value_targeted, nextWord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL is https://en.wikipedia.org/wiki/Dog\n",
      "Freq of targeted word: 0.25906735751295334\n",
      "| Word     |   Frequency |   Frequency Percentage |\n",
      "|----------+-------------+------------------------|\n",
      "| dogs     |         206 |                 4.8516 |\n",
      "| dog      |         125 |                 2.9439 |\n",
      "| wolves   |          31 |                 0.7301 |\n",
      "| humans   |          30 |                 0.7065 |\n",
      "| can      |          30 |                 0.7065 |\n",
      "| human    |          29 |                 0.683  |\n",
      "| breeds   |          27 |                 0.6359 |\n",
      "| domestic |          26 |                 0.6123 |\n",
      "| study    |          26 |                 0.6123 |\n",
      "| pet      |          26 |                 0.6123 |\n",
      "| years    |          20 |                 0.471  |\n",
      "| also     |          19 |                 0.4475 |\n",
      "| one      |          19 |                 0.4475 |\n",
      "| may      |          18 |                 0.4239 |\n",
      "| many     |          16 |                 0.3768 |\n",
      "| people   |          15 |                 0.3533 |\n",
      "| wolf     |          14 |                 0.3297 |\n",
      "| health   |          14 |                 0.3297 |\n",
      "| breed    |          13 |                 0.3062 |\n",
      "| found    |          13 |                 0.3062 |\n",
      "Next word to explore : lists\n"
     ]
    }
   ],
   "source": [
    "a, nw = scrapWiki(\"dog\", \"Yes\", \"animal\", True)\n",
    "print(\"Next word to explore : \" + str(nw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter valid string\n"
     ]
    }
   ],
   "source": [
    "# Sampling Exercise\n",
    "# Design a strategy to obtain samples\n",
    "target_word = \"information\"\n",
    "searchInWord = []\n",
    "rate, searchInWord = scrapWiki(searchInWord, \"Yes\", target_word, False)\n",
    "\n",
    "rates = []\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(rates, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
